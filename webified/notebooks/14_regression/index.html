---
layout: content
metadata: notebooks_14_regression_metadata
---
<h1>Regression</h1>
<h1>Contents</h1>
<h1>Introduction</h1>
<p>We have some data points and a number or a label is assigned to each data point. Our goal is to predict the number or label of an unseen data point after learning from the data we already have. We assume each data point is a vector $x$ and we want to predict $f(x)$. The first idea is to use interpolation. By using interpolation, we will have a high degree polynomial which fits our training data perfectly. But the problem is that, interpolation leads to overfitting. So the error for unseen data will be too large. In regression, we aim to find the best curve with lower degree. Although there will be some training error here, our test error will decrease since we are avoiding overfitting.</p>
<h1>1 - What is regression ?</h1>
<h2>Problem definition</h2>
<h1>2 - Linear Regression</h1>
<p>Here, we want to assign $f(x)$ to each data point $x$. In linear regression, we assume $f$ is a linear function. We can define $f$ as</p>
<p>$$
    f_w(x) = w^T x = \begin{bmatrix}
        w_0 &amp; w_1 &amp; \cdots &amp; w_n
    \end{bmatrix} . \begin{bmatrix}
        1 \ x_1 \ \vdots \ x_n
    \end{bmatrix}.
$$
We assumed $x_0 = 1$ in $x$ to have bias in our function. So the data points are in an n-dimentional space.</p>
<p>We also define $y$ as</p>
<p>$$
    y_w = \begin{bmatrix}
        f_w(x^{(1)}) \ f_w(x^{(2)}) \ \vdots \ f_w(x^{(m)})
    \end{bmatrix}
$$
where $x^{(i)}$ is the i'th data point. </p>
<h2>Loss Function</h2>
<p>After defining $f$, we need to find the best function. By defining a loss function, we can try to minimize loss by changing $w$ in the main function. Assuming $L$ as our loss function, best $f$ will be</p>
<p>$$
    \hat{w} = argmin_w L(y_w, \hat{y}) \rightarrow f_{best}(x) = f_{\hat{w}}(x) 
$$</p>
<p>where $\hat{y}$ is the given number for each data point.</p>
<h3>Mean Squared Error (MSE)</h3>
<p>The main loss function we use is mean squared error. It's defined as </p>
<p>$$
    MSE(y_w, \hat{y}) = \frac{1}{2} \Sigma_{i=1}^m \left[ y_w^{(i)} - \hat{y}^{(i)} \right]^2 .
$$
The main reason for using this function is that we can calculate gradient easily. So we can use gradient descent to find $\hat{w}$.</p>
<h2>Finding $\hat{w}$</h2>
<h3>Gradient Descent</h3>
<p>We want to use gradient descent to find $\hat{w}$. First, we need to calculate $\nabla_w L(y_w, \hat{y})$ because it's used in the gradient descent method. The partial derivitives for MSE are:</p>
<p>$$
    \frac{\partial MSE}{\partial w_j} = - \Sigma_{i = 1}^m x_j^{(i)} \left[ y_w^{(i)} - \hat{y}^{(i)} \right]
$$</p>
<p>So for gradinet, we have:</p>
<p>$$
    \nabla_w MSE = \begin{bmatrix}
        \frac{\partial MSE}{\partial w_0} \ \vdots \ \frac{\partial MSE}{\partial w_n}
    \end{bmatrix}
$$</p>
<p>Now, we can find $\hat{w}$ by the gradient descent algorithm as</p>
<p>$$
    w^{(i+1)} = w^{(i)} - \eta \nabla_{w^{(i)}} L
$$</p>
<p>where $\eta$ is the learning rate.</p>
<h3>Normal Equation</h3>
<p>If we define</p>
<p>$$
    X = \begin{bmatrix}
        x^{(1)} \ \vdots \ x^{(m)}
    \end{bmatrix}
$$</p>
<p>and solve the equation $\nabla_w MSE = 0$, we get the normal equation. It's defined as
$$
    \hat{w} = (X^TX)^{-1} X^T \hat{y}.
$$
However, for using this equation, our features must be linearly independent. Otherwise, $(X^TX)^{-1}$ is not defined. In that case we can use pseudo inverse of $X^TX$ instead of the $(X^TX)^{-1}$. Since the calculation of inverse is computationaly inefficient, we usually prefer using gradient descent. </p>
<h1>3 - Learning Curves Using Polynomials</h1>
<p>In this section, we want to find the best $P(x)$ where x is a real number and $P$ is an n'th degree polynomial.</p>
<h2>Reduction to Linear Regression</h2>
<p>We can define 
$$
    z = \begin{bmatrix}
        x^0 \ x^1 \ \vdots \ x^n
    \end{bmatrix},
$$
then use linear regression to find $f_w(z) = w^T z$. From definition, we know that $P(x) = f_w(z)$. However, if $n$ is too large, overfitting might happen since we are getting closer and closer to interpolation.</p>
<h2>Overfitting</h2>
<p>To prevent overfitting, we must try to define $n$ optimaly. Since $n$ is a hyperparameter here, we can use validation set.</p>
<h3>Using Validation Set (Held-Out Data)</h3>
<p>We split a part of the training data, and don't use it for training. Then, by calculating loss function over this data, we can optimize $n$. Since the model hasn't seen these data points, we can be sure that overfitting will decrease.</p>
<h3>Regularization</h3>
<p>Although using validation set is a good way to prevent overfitting, We still might be trying to find the curve which best fits the data, not the curve which best matches it. Regularization tries to make $w$ smaller. The intuition is that large coefficients in $w$ happen because it tries to fit the points. It means that the curve will only get closer to each point. However, decreasing coeffiecients will make a better curve which might be further from each point, but does a better job at predicting the unseen data.</p>
<p>We only define $l_2-\text{regularization}$ since it's easier to derive and understand. The loss function is </p>
<p>$$
    \tilde{E}(y_w, \hat{y}) = \frac{1}{2} \Sigma_{i=1}^m \left[ y_w^{(i)} - \hat{y}^{(i)} \right]^2 + \frac{\lambda}{2}||w||^2
$$
where $\lambda$ is hyperparameter which controls how small the $w$ should be.</p>
<p><strong><em>*</em></strong><strong> Examples </strong><strong><em>*</em></strong>****</p>
<h1>4 - Regularization</h1>
<h2>ridge (OPTIONAL)</h2>
<h2>lasso (OPTIONAL)</h2>
<h2>elastic (OPTIONAL)</h2>
<h1>5 - Logistic Regression</h1>
<h1>Conclusion</h1>
<h1>Refrences</h1>
<ul>
<li>https://en.wikipedia.org/wiki/Linear_regression</li>
<li>https://math.stackexchange.com/questions/1962877/compute-the-gradient-of-mean-square-error</li>
<li>https://towardsdatascience.com/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71</li>
</ul>